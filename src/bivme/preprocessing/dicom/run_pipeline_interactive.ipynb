{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import torch\n",
    "import shutil\n",
    "import time\n",
    "import importlib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from bivme.preprocessing.dicom.select_views import select_views\n",
    "from bivme.preprocessing.dicom.segment_views import segment_views\n",
    "from bivme.preprocessing.dicom.generate_contours import generate_contours\n",
    "from bivme.preprocessing.dicom.export_guidepoints import export_guidepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available (torch)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code reads in DICOM files and generates GPFiles for personalised biventricular mesh fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Set up directories\n",
    "The preprocessing pipeline expects DICOM files to be organised in a certain way. There are three key things to get right with the DICOMs before running the code. \n",
    "\n",
    "Firstly, there should be no non-cine images or non-cardiac views. My job is hard enough as it is. Some day I'll add a pre-preprocessing step, but not today.\n",
    "\n",
    "Secondly, images should be separated into folders by case, like so:\n",
    "\n",
    "    input_path   \n",
    "    └─── case1\n",
    "        │─── *.dcm\n",
    "    └─── case2\n",
    "        │─── *.dcm\n",
    "    └─── ...\n",
    "    \n",
    "Thirdly, all images should be converted to .dcm format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\jdil469\\Code\\biv-me\\src\\bivme\\preprocessing\\dicom\n"
     ]
    }
   ],
   "source": [
    "batch_ID = 'test' # This will serve as your output folder name. Example: 'test'\n",
    "analyst_id = 'analyst1' # Example: 'analyst1'\n",
    "input_path = '' # Path to the input DICOM folder\n",
    "processed_path = '' # Path to the processed folder, where view predictions and segmentations will be stored. This will be created upon run time.\n",
    "states_path = '' # Path to the states folder, where the logs and view predictions will be stored for reference. This will be created upon run time.\n",
    "output_path = '' # Path to the output folder, where GP files will be stored. This will be created upon run time.\n",
    "\n",
    "# Target path: src/bivme/preprocessing/dicom/models\n",
    "cwd = os.getcwd()\n",
    "print('Current working directory:', cwd)\n",
    "MODEL_DIR = os.path.join(cwd,'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0.5: Choose case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = '' # Enter the case ID. This should be a subfolder within the input_path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = os.path.join(input_path)\n",
    "dst = os.path.join(processed_path, batch_ID)\n",
    "states = os.path.join(states_path, batch_ID)\n",
    "output = os.path.join(output_path, batch_ID)\n",
    "\n",
    "os.makedirs(dst, exist_ok=True)\n",
    "os.makedirs(states, exist_ok=True)\n",
    "os.makedirs(output, exist_ok=True)\n",
    "\n",
    "case_src = os.path.join(src, case)\n",
    "if not os.path.isdir(case_src):\n",
    "    print(f'Case {case} not found in source folder. Please check the case ID.')\n",
    "    sys.exit()\n",
    "\n",
    "case_dst = os.path.join(dst, case)\n",
    "\n",
    "if os.path.exists(case_dst):\n",
    "    enter = input(f'Case {case} already processed. Do you want to overwrite? (y/n): ')\n",
    "    if enter == 'y':\n",
    "        shutil.rmtree(case_dst)\n",
    "    else:\n",
    "        print(f'Change the case ID or delete the existing folder {case_dst} before proceeding.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing case: cardiohance_001\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Create log file to record some details\n",
    "states = os.path.join(states, case, analyst_id)\n",
    "os.makedirs(states, exist_ok=True)\n",
    "if not os.path.exists(os.path.join(states, 'logs')):\n",
    "    os.mkdir(os.path.join(states, 'logs'))\n",
    "\n",
    "log_name = f'{case}_{analyst_id}_{time.ctime().replace(\" \",\"-\").replace(\":\",\"-\")}.txt'\n",
    "log_path = os.path.join(states, 'logs', log_name)\n",
    "with open(log_path, 'w') as f:\n",
    "    f.write(f'Case: {case}\\n')\n",
    "    f.write(f'Batch ID: {batch_ID}\\n')\n",
    "    f.write(f'Source: {case_src}\\n')\n",
    "    f.write(f'Destination: {dst}\\n')\n",
    "    f.write(f'Analyst ID: {analyst_id}\\n')\n",
    "    f.write(f'Processing started at: {time.ctime()}\\n')\n",
    "    f.write('\\n')\n",
    "\n",
    "print(f'Processing case: {case}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules to ensure any changes are reflected\n",
    "importlib.reload(sys.modules[select_views.__module__])\n",
    "from select_views import select_views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: View classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared for view prediction. 14 image series found.\n",
      "Running view predictions...\n",
      "No duplicate slice locations found.\n",
      "Multiple series classed as 3ch.\n",
      "Excluded series [15]\n",
      "View predictions for cardiohance_001:\n",
      "SAX-atria: 1 series\n",
      "SAX: 5 series\n",
      "4ch: 1 series\n",
      "3ch: 1 series\n",
      "Excluded: 1 series\n",
      "2ch: 1 series\n",
      "2ch-RT: 1 series\n",
      "LVOT: 1 series\n",
      "RVOT: 1 series\n",
      "RVOT-T: 1 series\n",
      "View selection complete.\n",
      "Number of phases: 27\n"
     ]
    }
   ],
   "source": [
    "option = 'default' # Either 'default' or 'load'. Default will automatically perform view prediction. Load will load the view predictions from the states folder.\n",
    "\n",
    "slice_info_df, num_phases, slice_mapping = select_views(case, case_src, case_dst, MODEL_DIR, states, option)\n",
    "print('View selection complete.')\n",
    "\n",
    "print(f'Number of phases: {num_phases}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually correct view predictions (optional)\n",
    "No view prediction network is perfect. You can manually correct the view predictions if needed. The view predictions will be saved in:\n",
    "\n",
    "    states_path   \n",
    "    └───batch_ID\n",
    "        └───case\n",
    "            └───analyst_id\n",
    "                |───view_predictions.csv\n",
    "\n",
    "You can visually inspect which images have been classified as which view (or excluded) here:\n",
    "\n",
    "\n",
    "    processed_path   \n",
    "    └───batch_ID\n",
    "        └───case\n",
    "            └───view-classsification\n",
    "                └───sorted\n",
    "\n",
    "If you do correct the views, make run the cell block below with the variable 'option' set to 'load'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = 'load' # Either 'default' or 'load'. Default will automatically perform view prediction. Load will load the view predictions from the states folder.\n",
    "\n",
    "slice_info_df, num_phases, slice_mapping = select_views(case, case_src, case_dst, MODEL_DIR, states, option)\n",
    "print('View selection complete.')\n",
    "\n",
    "print(f'Number of phases: {num_phases}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules to ensure any changes are reflected\n",
    "importlib.reload(sys.modules[segment_views.__module__])\n",
    "from segment_views import segment_views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing SAX images to nifti files...\n",
      "\n",
      "Segmenting SAX images...\n",
      "\n",
      "*** Making predictions for SAX images ***\n",
      "There are 5 cases in the source folder\n",
      "I am process 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 5 cases that I would like to predict\n",
      "\n",
      "Predicting SAX_3d_10:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with SAX_3d_10\n",
      "\n",
      "Predicting SAX_3d_11:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with SAX_3d_11\n",
      "\n",
      "Predicting SAX_3d_12:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with SAX_3d_12\n",
      "\n",
      "Predicting SAX_3d_8:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with SAX_3d_8\n",
      "\n",
      "Predicting SAX_3d_9:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with SAX_3d_9\n",
      "Done with SAX\n",
      "\n",
      "Writing 2ch images to nifti files...\n",
      "\n",
      "Segmenting 2ch images...\n",
      "\n",
      "*** Making predictions for 2ch images ***\n",
      "There are 1 cases in the source folder\n",
      "I am process 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 1 cases that I would like to predict\n",
      "\n",
      "Predicting 2ch_3d_16:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with 2ch_3d_16\n",
      "Done with 2ch\n",
      "\n",
      "Writing 3ch images to nifti files...\n",
      "\n",
      "Segmenting 3ch images...\n",
      "\n",
      "*** Making predictions for 3ch images ***\n",
      "There are 1 cases in the source folder\n",
      "I am process 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 1 cases that I would like to predict\n",
      "\n",
      "Predicting 3ch_3d_14:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with 3ch_3d_14\n",
      "Done with 3ch\n",
      "\n",
      "Writing 4ch images to nifti files...\n",
      "\n",
      "Segmenting 4ch images...\n",
      "\n",
      "*** Making predictions for 4ch images ***\n",
      "There are 1 cases in the source folder\n",
      "I am process 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 1 cases that I would like to predict\n",
      "\n",
      "Predicting 4ch_3d_13:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with 4ch_3d_13\n",
      "Done with 4ch\n",
      "\n",
      "Writing RVOT images to nifti files...\n",
      "\n",
      "Segmenting RVOT images...\n",
      "\n",
      "*** Making predictions for RVOT images ***\n",
      "There are 1 cases in the source folder\n",
      "I am process 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 1 cases that I would like to predict\n",
      "\n",
      "Predicting RVOT_3d_19:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with RVOT_3d_19\n",
      "Done with RVOT\n",
      "\n",
      "Segmentation complete. Time taken: 95.30491971969604 seconds (3d version).\n"
     ]
    }
   ],
   "source": [
    "## Segmentation\n",
    "version = '3d' # '2d' or '3d' segmentation models. 3D is recommended for better efficiency and temporal coherence across frames.\n",
    "start_time = time.time()\n",
    "segment_views(case, case_dst, MODEL_DIR, slice_info_df, version = version)\n",
    "end_time = time.time()\n",
    "print(f'Segmentation complete. Time taken: {end_time-start_time} seconds ({version} version).')\n",
    "\n",
    "# Add segmentation time to log\n",
    "with open(log_path, 'a') as f:\n",
    "    f.write(f'Segmentation time: {end_time-start_time} seconds ({version} version).\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.5: Review segmentations (optional)\n",
    "Images and segmentations are stored here in nifti format (.nii.gz). \n",
    "\n",
    "    processed_path   \n",
    "    └───batch_ID\n",
    "        └───case\n",
    "\n",
    "\n",
    "Hopefully it won't be necessary 99% of the time, but, if you wish, segmentations can be corrected here, and the rest of the code will incorporate those changes. \n",
    "\n",
    "By default, the contouring code (after segmentation) carries out some basic QC. All label types (except for the RV myocardium) have all but their largest components removed, so you shouldn't need to worry about fixing minor things such as removing extraneous label regions. If you find a problem with the exported contours or fitted models, it's more likely due to poor RV myo segmentation or poor segmentation around valve planes.   \n",
    "\n",
    "3D Slicer or ITKSnap are good tools correcting segmentations. If you do, make sure to only change the segmentations with _2d_ in the name. These are the ones that get loaded later on for contouring & exporting. Both versions of segmentation model will output these _2d_ segmentation files. Unfortunately, this also means correcting segmentations frame by frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules to ensure any changes are reflected\n",
    "importlib.reload(sys.modules[generate_contours.__module__])\n",
    "from generate_contours import generate_contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating contours for SAX slice 8...\n",
      "\n",
      "Generating contours for SAX slice 9...\n",
      "\n",
      "Generating contours for SAX slice 10...\n",
      "\n",
      "Generating contours for SAX slice 11...\n",
      "\n",
      "Generating contours for SAX slice 12...\n",
      "\n",
      "Generating contours for 2ch slice 16...\n",
      "\n",
      "Generating contours for 3ch slice 14...\n",
      "\n",
      "Generating contours for 4ch slice 13...\n",
      "\n",
      "Generating contours for RVOT slice 19...\n",
      "\n",
      "Guide points generated.\n"
     ]
    }
   ],
   "source": [
    "slice_dict = generate_contours(case, case_dst, slice_info_df, num_phases, version = version)\n",
    "print('Guide points generated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload to ensure any changes are reflected\n",
    "importlib.reload(sys.modules[export_guidepoints.__module__])\n",
    "from export_guidepoints import export_guidepoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Export guidepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export complete.\n",
      "Case cardiohance_001 complete.\n",
      "Total time taken: 102.80258440971375 seconds.\n"
     ]
    }
   ],
   "source": [
    "export_guidepoints(case, case_dst, output, slice_dict, slice_mapping)\n",
    "print('Export complete.')\n",
    "print(f'Case {case} complete.')\n",
    "print(f'Total time taken: {time.time()-start_time} seconds.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAP_ABI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
